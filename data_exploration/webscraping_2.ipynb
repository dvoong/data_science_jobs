{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Webscraping 2\n",
    "\n",
    "So it's been about a month since I started this project and I've only really had three days where I've had time to work on it. Because work is so intermittent I decided that the next task should be to automate the webscraping so that when I do come back to the project I will have some more data. The website ([ftp://ftp.zois.co.uk/pub/jcp](ftp://ftp.zois.co.uk/pub/jcp)) where I originally got my data seems to be down for good so I'll have to rely on my own webscraper.\n",
    "\n",
    "First to prototype out what the script will look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Check when the last scraping was done\n",
    "    Check the current date and time\n",
    "    Depending on when the last scrape was, \n",
    "    scrape the dates needed to update to now\n",
    "    The job match website allows you to search all, today, yesterday, 3, 7, 14, 30 days ago.\n",
    "    The url string has a tm parameter, tm=-1 for all, tm=0 for today, tm=1 for yesterday,\n",
    "    tm=3 for 3 days ago and so on\n",
    "    On a side note: weirdly enough if you reload the page continually the number of results change continually\n",
    "    e.g. if you reload the URL: https://jobsearch.direct.gov.uk/JobSearch/PowerSearch.aspx?redirect=http%3a%2f%2fjobsearch.direct.gov.uk%2fhome.aspx&pp=25&q=Data%20Science&sort=rv.dt.di&re=134&tm=3\n",
    "    the number of results change\n",
    "    If the last scraping was done on a different day from today\n",
    "    Then scrape everything between that date and now, e.g. if the last scrape was two days ago,\n",
    "    then scrape everything using the 3 days ago filter.\n",
    "    If the last scrape was yesterday, scrape everything using the \"yesterday\" filter\n",
    "    \n",
    "    How to keep track of when the last scrape was?\n",
    "    Need some form of permanant storage,\n",
    "    candidates are\n",
    "    - using a file, e.g. a text file which stores a date\n",
    "    - use a database table\n",
    "    \n",
    "    I think I'll go with a table, that way I can store some meta data like how many jobs were added per scrape.\n",
    "    Each row in the table will correspond to a scraping session,\n",
    "    Columns in the table might include stuff like, date and time of scrape, number of items imported,\n",
    "    that's all I can actually think of right now\n",
    "    \n",
    "    Once we know when the last scrape was scrape all the data from then until now\n",
    "    Have the script sleep for a while depending on how often you want to scrape\n",
    "    Then scrape repeatedly\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now attempt to write it out in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_science_jobs.models import ScrapingSession # This doesn't actually exist yet\n",
    "\n",
    "last_session = ScrapingSession.objects.last()\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "days_since_last_scrape = None if last_session == None else now.date() - last_session.date\n",
    "# a scraping session will need to have a data attribute\n",
    "\n",
    "if days_since_last_scrape == 0:\n",
    "    date_filter = 0\n",
    "elif days_since_last_scrape == 1:\n",
    "    date_filter = 1\n",
    "elif days_since_last_scrape <= 3:\n",
    "    date_filter = 3\n",
    "elif days_since_last_scrape <= 7:\n",
    "    date_filter = 7\n",
    "elif days_since_last_scrape <= 14:\n",
    "    date_filter = 14\n",
    "elif days_since_last_scrape <= 30:\n",
    "    date_filter = 30\n",
    "else:\n",
    "    date_filter = -1\n",
    "    \n",
    "SCRAPING_URL = 'https://jobsearch.direct.gov.uk/JobSearch/PowerSearch.aspx?redirect=http%3A%2F%2Fjobsearch.direct.gov.uk%2Fhome.aspx&pp=25&sort=rv.dt.di&re=3&tm={date_filter}&pg={page_number}&q=%22Data%20Science%22'    \n",
    "\n",
    "class JobScraper:\n",
    "    \n",
    "    FREQUENCY = datetime.timedelta(hours=1)\n",
    "    def scrape(self):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "scraper = JobScraper()\n",
    "\n",
    "while True:\n",
    "    scraper.scrape()\n",
    "    import time\n",
    "    time.sleep(scraper.FREQUENCY.total_seconds())\n",
    "    \n",
    "    # Need to update the date filter here so the next time it runs it only updates todays jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It started to get a bit messy here so I decided to rewrite it, hopefully with each iteration I'll get something that is readable, maintainable, correct and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named data_science_jobs.models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ec560e70d627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_science_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScrapingSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mSCRAPING_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://jobsearch.direct.gov.uk/JobSearch/PowerSearch.aspx?redirect=http%3A%2F%2Fjobsearch.direct.gov.uk%2Fhome.aspx&pp=25&sort=rv.dt.di&re=3&tm={date_filter}&pg={page_number}&q=%22Data%20Science%22'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named data_science_jobs.models"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "from data_science_jobs.models import ScrapingSession\n",
    "\n",
    "SCRAPING_URL = 'https://jobsearch.direct.gov.uk/JobSearch/PowerSearch.aspx?redirect=http%3A%2F%2Fjobsearch.direct.gov.uk%2Fhome.aspx&pp=25&sort=rv.dt.di&re=3&tm={date_filter}&pg={page_number}&q=%22Data%20Science%22'    \n",
    "FREQUENCY = datetime.timedelta(hours=1)\n",
    "\n",
    "def get_date_filter(last_session):\n",
    "    now = datetime.datetime.now()\n",
    "    today = now.date()\n",
    "    days_since_last_scrape = None if last_session == None else today - last_session.date\n",
    "    if days_since_last_scrape == 0:\n",
    "        date_filter = 0\n",
    "    elif days_since_last_scrape == 1:\n",
    "        date_filter = 1\n",
    "    elif days_since_last_scrape <= 3:\n",
    "        date_filter = 3\n",
    "    elif days_since_last_scrape <= 7:\n",
    "        date_filter = 7\n",
    "    elif days_since_last_scrape <= 14:\n",
    "        date_filter = 14\n",
    "    elif days_since_last_scrape <= 30:\n",
    "        date_filter = 30\n",
    "    else:\n",
    "        date_filter = -1\n",
    "    return date_filter\n",
    "\n",
    "def get_n_pages(response):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "def populate_db(response):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "while True:\n",
    "    last_session = ScrapingSession.objects.last()\n",
    "    date_filter = get_date_filter(last_session)\n",
    "    response = requests.get(SCRAPING_URL.format(page_number=1, date_filter=date_filter))\n",
    "    n_pages = get_n_pages(response)\n",
    "    populate_db(response)\n",
    "    for page in range(2, n_pages):\n",
    "        response = requests.get(SCRAPING_URL.format(page_number=page, date_filter=date_filter))\n",
    "        populate_db(response)\n",
    "    time.sleep(FREQUENCY.total_seconds())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to get rid of the `JobScraper` class, don't think I really need to use objects in this simple script.\n",
    "\n",
    "This version looks pretty good to me, now I'll finish the `get_n_pages` function, I already wrote some of this in the previous notebook, I'll just copy and paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "\n",
    "def get_n_pages(response):\n",
    "    tree = html.fromstring(response.content)\n",
    "    page_summary = tree.cssselect(\"div.pagesSummary span\")\n",
    "    n_pages = int(page_summary.split(' ')[-1])\n",
    "    return n_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for populating the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_science_jobs import Job\n",
    "\n",
    "def populate_db(response):\n",
    "    failures = []\n",
    "    jobs = []\n",
    "    tree = html.fromstring(response.content)\n",
    "    table = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[7]/table')[0]\n",
    "    job_links = table.findall(\"tr\")[1:] # remove headers\n",
    "    job_links = [row.findall(\"td\")[2].find(\"a\").attrib['href'] for row in job_links]\n",
    "    for job_link in job_links:\n",
    "        response = requests.get(job_link)\n",
    "        tree = html.fromstring(response.content)\n",
    "        try:\n",
    "            job = {}\n",
    "            job['jobid'] = int(tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[4]/div/div[4]/dl/dd[1]')[0].text)\n",
    "            job['title'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[5]/div[2]/h2[2]')[0].text\n",
    "            job['description'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[5]/div[2]/div[1]')[0].text\n",
    "            try:\n",
    "                job['company'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[5]/div[2]/h2[1]')[0].text\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                job['apply_'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[5]/div[2]/div[2]/a')[0].attrib['href']\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                job['added'] = dateutil.parser.parse(tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[4]/div/div[4]/dl/dd[2]')[0].text)\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                job['location'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[4]/div/div[4]/dl/dd[3]')[0].text\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                job['industry'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[4]/div/div[4]/dl/dd[4]')[0].text\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                job['job_type'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[4]/div/div[4]/dl/dd[5]')[0].text\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                job['salary'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[4]/div/div[4]/dl/dd[6]')[0].text\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                job['hours_of_work'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[4]/div/div[4]/dl/dd[7]')[0].text # not in the original data model\n",
    "            except IndexError:\n",
    "                pass\n",
    "            try:\n",
    "                job['job_reference_code'] = tree.xpath('//*[@id=\"aspnetForm\"]/div/div[2]/div[4]/div/div[4]/dl/dd[8]')[0].text # not in original data model\n",
    "            except IndexError:\n",
    "                pass\n",
    "            jobs.append(job)\n",
    "        except Exception as e:\n",
    "            print 'failure:', job_link\n",
    "            failures.append((job_link, e),)\n",
    "            \n",
    "        for job in jobs:\n",
    "            job = JobListing(**job)\n",
    "            job.save()\n",
    "            \n",
    "        return {\n",
    "            'jobs': jobs,\n",
    "            'failures': failures\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bear in mind that none of this code has actually been tested to see if it works! I'm actually certain that it won't work because, for example, I'm pretty sure my JobListing class doesn't know how to deal with the `hours_of_work` attribute and the `data_science_jobs` module doesn't event have a `JobListing` class in it, nor a `ScrapingSession`, the previous `JobListing` class lives in the `data_exploration` module which was as its name suggest for exploration only. Now I have a better idea of what I want and how to structure it I'll start coding it up properly with tests and all. Until next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
